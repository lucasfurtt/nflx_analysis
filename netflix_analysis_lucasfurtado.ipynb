{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NFLX Exercise - Analysis\n",
        "\n",
        "Hi!\n",
        "\n",
        "The intention of this notebook code is to answear questions about a Netflix database to our client.  I'll separate the notebook cells to make more easier to identify what question will be answeared now; also, I'll explain the logic in the code.\n",
        "Questions:\n",
        " 1. In the ‘Films (English)’ category, which film has the most appearances in our data set\n",
        " (NFLX Top 10 tab of the Sheet)? What were its average weekly hours viewed?\n",
        " 2. In the 'Films (English)' category, which film has the lowest IMDb rating? What were its\n",
        " average weekly hours viewed?\n",
        " 3. In the 'Films (Non-English)' category, which film has spent the most weeks in the top 10?\n",
        " To estimate the number of users who watched this film, what assumptions would you\n",
        " make andwhat risks are involved?\n",
        " ○ Pleaselimit your response to 150 words or less.\n",
        " 4. Whatarethe risks of ignoring the data from the week of May 22nd when calculating the\n",
        " metrics from the previous questions?\n",
        " ○ Pleaselimit your response to 150 words or less.\n",
        " 5. While we've indicated that the 'weekly_hours_viewed' data for the week of May 22nd\n",
        " cannot be used in our estimates, we may want to fill in this missing information for other\n",
        " analyses. As a Data Specialist, what methodology would you propose to estimate the\n",
        " 'weekly_hours_viewed' for this missing week?"
      ],
      "metadata": {
        "id": "a9KEddkMROWR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYk-nS4vRCLH"
      },
      "outputs": [],
      "source": [
        "#first of all, we should import pandas to read the dataset properly.\n",
        "import pandas as pd\n",
        "df = pd.read_excel('/content/Data Specialist NFLX Data.xlsx', sheet_name=\"NFLX Top 10\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#as explained in the text, we have incomplete data from the week of may 2022. I will load another dataset that excludes this week.\n",
        "filtered_df = df[df['week'] != '2022-05-22']"
      ],
      "metadata": {
        "id": "zodhba0YYm_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now we can go to the 1st question. for that, I'll filter the films that belongs to the english category and after that count its appearances in the dataset. with its appearances, I can make an average using the previously count.\n",
        "english_films = filtered_df[filtered_df['category'] == 'Films (English)']\n",
        "\n",
        "#we can simply use value_counts to count the appearances of show title, and after that use idxmax to return the index of the counting method.\n",
        "count_films = english_films['show_title'].value_counts()\n",
        "count_films_title = english_films['show_title'].value_counts().idxmax()\n",
        "count_films_max = english_films['show_title'].value_counts().max()\n",
        "print('Film with most weekly appearances: ',count_films_title)\n",
        "\n",
        "#now we know that 'Sonic the Hedgehog' is the film with most appearances. to know the weekly hours watched avg, we can sum the weekly hours viewed and divide by the weekly appearances.\n",
        "hours_viewed_titles = english_films.groupby('show_title')['weekly_hours_viewed'].sum()\n",
        "hours_viewed_max_count = hours_viewed_titles[count_films_title]\n",
        "avg_hours_viewed = hours_viewed_max_count/count_films_max\n",
        "print('Avearage of watched hours: ',avg_hours_viewed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEZv3ZVEZxRf",
        "outputId": "93946cc0-cfa9-4b51-d35c-28713357b5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Film with most weekly appearances:  Sonic the Hedgehog\n",
            "Avearage of watched hours:  8550000.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to answear the 2nd question, we have to make a join between 'NFLX Top 10' sheet and 'IMDB Rating' sheet. the key that will be used is 'show_title' and 'title'. in this case, we can bring the rating column to previously used df.\n",
        "imdb_df = pd.read_excel('/content/Data Specialist NFLX Data.xlsx', sheet_name=\"IMDB Rating\")\n",
        "english_films_merge = english_films.merge(imdb_df[['title', 'rating']],\n",
        "                                    how='left',\n",
        "                                    left_on='show_title',\n",
        "                                    right_on='title')\n",
        "\n",
        "#we can drop the 'title' column from imdb rating, because we already have this key in the left dataset.\n",
        "english_films_merge.drop(columns=['title'], inplace=True)\n",
        "\n",
        "#before find out the lowest rating, we have to exclude the rating 0 in our dataset. according to the note, if a rating is unavailable on IMDb, we input it as 0 and exclude it from any ratings analyses.\n",
        "english_films_merge_rating = english_films_merge[english_films_merge['rating'] != 0]\n",
        "\n",
        "#now we can find out the lowest rating film on imdb. we will use the min function.\n",
        "min_rating = english_films_merge_rating['rating'].min()\n",
        "min_rating_title = english_films_merge_rating[english_films_merge_rating['rating'] == min_rating]['show_title'].values[0]\n",
        "print('Film with lowest IMDB rating: ',min_rating_title)\n",
        "\n",
        "#now we know that '365 Days: This Day' is the film with the lowest IMDB rating. to know the weekly hours watched avg, we can sum the weekly hours viewed and divide by the weekly appearances. we can use the same sum variable.\n",
        "hours_viewed_min_sum = hours_viewed_titles[min_rating_title]\n",
        "count_films_lowest = english_films['show_title'].value_counts()[min_rating_title]\n",
        "avg_hours_viewed_lowest_rating = hours_viewed_min_sum/count_films_lowest\n",
        "print('Avearage of watched hours: ',avg_hours_viewed_lowest_rating)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMxfEsHUhPkH",
        "outputId": "bcc7d74d-e98f-499d-d9cf-dd25d031e672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Film with lowest IMDB rating:  365 Days: This Day\n",
            "Avearage of watched hours:  38696666.666666664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#in the 3rd question, we need change the filter of the dataset to non english filmes and return the max value of \"cumulative_weeks_in_top_10\" column.\n",
        "non_english_films = filtered_df[filtered_df['category'] == 'Films (Non-English)']\n",
        "\n",
        "#returning the max value of \"cumulative_weeks_in_top_10\" and its title.\n",
        "non_english_max_appearances = non_english_films['cumulative_weeks_in_top_10'].max()\n",
        "title_non_english_max_appearances = non_english_films[non_english_films['cumulative_weeks_in_top_10'] == non_english_max_appearances]['show_title'].values[0]\n",
        "print('Non english film with most weekly appearances: ',title_non_english_max_appearances)\n",
        "\n",
        "#sum of weekly hours views in 'Through My Window' film\n",
        "hours_viewed_tmw = df[df['show_title'] == title_non_english_max_appearances]['weekly_hours_viewed'].sum()\n",
        "print('Total watched hours of Through my Window: ',hours_viewed_tmw)\n",
        "\n",
        "#runtime of Through my Window\n",
        "runtime_df = pd.read_excel('/content/Data Specialist NFLX Data.xlsx', sheet_name=\"Runtime\")\n",
        "runtime_tmw = (runtime_df[runtime_df['title']==title_non_english_max_appearances]['runtime'].values[0])/60\n",
        "print('Runtime of Through my Window in hours:', runtime_tmw)\n",
        "\n",
        "#if we divide the sum of total hours that users spent watching this film with the runtime of the film, we can estimate the number of people that watched the movie\n",
        "people_watched_tmw = hours_viewed_tmw/runtime_tmw\n",
        "print('Estimated number of people that watched the movie:', people_watched_tmw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08GNBYsJp20h",
        "outputId": "5f47a12b-cf4e-4932-d7b4-6a1e0b3952f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non english film with most weekly appearances:  Through My Window\n",
            "Total watched hours of Through my Window:  10380000\n",
            "Runtime of Through my Window in hours: 1.9333333333333333\n",
            "Estimated number of people that watched the movie: 5368965.517241379\n"
          ]
        }
      ]
    }
  ]
}